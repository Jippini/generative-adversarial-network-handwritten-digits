{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN that generates handwritten digits\n",
    "\n",
    "A small Jupyter Notebook implementing a generative adversarial network (GAN) in Tensorflow that learns to generate images similar to the MNSIT dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T10:15:54.565824Z",
     "start_time": "2018-03-14T10:15:49.809955Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# import the MNIST dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# import the dataset\n",
    "mnist = input_data.read_data_sets('MNIST_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T10:15:54.857714Z",
     "start_time": "2018-03-14T10:15:54.568240Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# display the image\n",
    "image = mnist.train.images[1].reshape((28, 28))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator\n",
    "\n",
    "The discriminator has the job to differentiate if an image is generated or a real sample from the dataset. \n",
    "Discriminator architecture:\n",
    "\n",
    "\n",
    "1. **```Input```**: accepts 28px x 28px monochromatic images\n",
    "2. **```1st convolution```**: filtersize 5x5, 32 features, ```SAME``` padding, ReLU Activation \n",
    "3. **```Average Pooling```**: 2x2 kernel, 2x2 stride\n",
    "4. **```2nd convolution```**: fitersize 5x5, 64 features, ```SAME``` padding, ReLU Activation\n",
    "5. **```Average Pooling```**: 2x2 kernel, 2x2 stride\n",
    "6. **```1st fully-connected```**: 1024 neurons, ReLU activation\n",
    "7. **```2nd fully-connected```**: $K$ neurons, sigmoid activation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T10:15:54.960658Z",
     "start_time": "2018-03-14T10:15:54.859486Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def discriminator(image, reuseVariables):\n",
    "    with tf.variable_scope(\"discriminator\") as scope:\n",
    "        if (reuseVariables):\n",
    "            scope.reuse_variables()\n",
    "        \n",
    "        # 1st convolution\n",
    "        c1_out = tf.layers.conv2d(\n",
    "            inputs=image,\n",
    "            filters=32,\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "            name=\"d_c1\"\n",
    "        )       \n",
    "        \n",
    "        # Average pooling\n",
    "        p1_out = tf.layers.average_pooling2d(inputs=c1_out, pool_size=[2, 2], strides=[2,2], name=\"d_p1\")\n",
    "        \n",
    "        # 2nd convolution\n",
    "        c2_out = tf.layers.conv2d(\n",
    "            inputs=p1_out,\n",
    "            filters=64,\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "            name=\"d_c2\"\n",
    "        )\n",
    "        \n",
    "        # Average pooling\n",
    "        p2_out = tf.layers.average_pooling2d(inputs=c2_out, pool_size=[2, 2], strides=[2,2], name=\"d_p2\")\n",
    "\n",
    "        # 1st fully-connected\n",
    "        fc1_flat = tf.reshape(p2_out, [-1, 7 * 7 * 64])\n",
    "        fc1_out = tf.layers.dense(inputs=fc1_flat, units=1024, activation=tf.nn.relu, name=\"d_fc1\")\n",
    "\n",
    "        # 2nd fully-connected\n",
    "        fc2_out = tf.layers.dense(inputs=fc1_out, units=1, activation=None, name=\"d_fc2\")\n",
    "\n",
    "        return fc2_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "The generator has the job to produce realistic looking images to ultimatley deceive the discriminator.\n",
    "\n",
    "Generator architecture:\n",
    "\n",
    "1. **```Input```**: accepts a noise vector of length $N$\n",
    "2. **```1st fully-connected```**: upsamples the input vector to length $L$, applies ReLU activation function\n",
    "3. **```1st convolutional```**: filtersize 3x3, 2x2 strides, $N/2$ features, ```SAME``` padding, ReLU activation\n",
    "4. **[```batch normalization```](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm)**: using the default parameters\n",
    "5. **```upsample```**: factor 2x2, bilinear\n",
    "5. **```2nd convolutional```**: filtersize 3x3, 2x2 strides, $N/4$ features, ```SAME``` padding, ReLU activation\n",
    "7. **```batch normalization```**: using the default parameters\n",
    "8. **```upsample```**: factor 2x2, bilinear\n",
    "9. **```3rd convolutional```**: filtersize 3x3, 2x2 strides, 1 feature, sigmoid activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def generator(batch_size, noise_dim):\n",
    "    with tf.variable_scope(\"generator\") as scope:\n",
    "        \n",
    "        # Input - Noise vector\n",
    "        z = tf.random_normal(\n",
    "            [batch_size, noise_dim],\n",
    "            mean=0.0,\n",
    "            stddev=1.0,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "        # 1st fully-connected\n",
    "        fc1_layer = tf.layers.dense(inputs=z, units=3136, activation=tf.nn.relu, name='gz_dense')\n",
    "        fc1_out = tf.reshape(fc1_layer, (batch_size, 56, 56, 1)) # reshaping\n",
    "        \n",
    "        # 1st convolution\n",
    "        c1_layer = tf.layers.conv2d(\n",
    "            inputs=fc1_out,\n",
    "            filters=noise_dim/2,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[2, 2],\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "            activation=tf.nn.relu,\n",
    "            name='gz_conv1',\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        c1_out = tf.contrib.layers.batch_norm(inputs= c1_layer) # batch normalization\n",
    "\n",
    "        c1_out = tf.image.resize_images(images=c1_out, size=(56, 56)) # upsampling\n",
    "        \n",
    "        # 2nd convolution\n",
    "        c2_layer = tf.layers.conv2d(\n",
    "            inputs=c1_out,\n",
    "            filters=noise_dim/4,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[2, 2],\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "            activation=tf.nn.relu,\n",
    "            name='gz_conv2',\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        c2_out = tf.contrib.layers.batch_norm(inputs=c2_layer)  # batch normalization\n",
    "\n",
    "        c2_out = tf.image.resize_images(images=c2_out, size=(56, 56)) # upsampling\n",
    "\n",
    "        # 3rd convolution\n",
    "        c3_layer = tf.layers.conv2d(\n",
    "            inputs=c2_out,\n",
    "            filters=1,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[2, 2],\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "            activation=tf.nn.sigmoid,\n",
    "            name='gz_conv3',\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        return c3_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the GAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T10:26:24.096011Z",
     "start_time": "2018-03-14T10:26:23.824272Z"
    }
   },
   "outputs": [],
   "source": [
    "noise_dim = 100\n",
    "batch_size = 50\n",
    "\n",
    "with tf.variable_scope(\"input\"):\n",
    "    x = tf.placeholder(tf.float32, shape=(None,28,28,1))\n",
    "\n",
    "Gz = generator(batch_size, noise_dim)\n",
    "Dx = discriminator(x, False)\n",
    "Dg = discriminator(Gz, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T10:51:56.002525Z",
     "start_time": "2018-03-14T10:51:55.949165Z"
    }
   },
   "outputs": [],
   "source": [
    "# cross-entropy for the generator\n",
    "gz_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits = Dg, labels = tf.ones_like(Dg))\n",
    "gz_loss = tf.reduce_mean (gz_loss)\n",
    "\n",
    "# cross-entropy for the real-data-discriminator\n",
    "dx_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits = Dx, labels = tf.ones_like(Dx))\n",
    "dx_loss = tf.reduce_mean (dx_loss)\n",
    "\n",
    "# cross-entropy for the generated-data-discriminator\n",
    "dg_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits = Dg, labels = tf.zeros_like(Dg))\n",
    "dg_loss = tf.reduce_mean (dg_loss)\n",
    "\n",
    "# combined loss of the discriminators\n",
    "d_loss = dg_loss + dx_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T12:52:00.755627Z",
     "start_time": "2018-03-14T12:51:59.784876Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the trainable variables\n",
    "d_variables = tf.trainable_variables(\"discriminator\")\n",
    "gz_variables = tf.trainable_variables(\"generator\")\n",
    "\n",
    "# initialise the AdamOptimizers\n",
    "d_optimizer = tf.train.AdamOptimizer(0.0003)\n",
    "gz_optimizer = tf.train.AdamOptimizer(0.0001)\n",
    "\n",
    "d_minimizer = d_optimizer.minimize(d_loss, var_list=d_variables)\n",
    "gz_minimizer = d_optimizer.minimize(gz_loss, var_list=gz_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T12:52:59.868943Z",
     "start_time": "2018-03-14T12:52:59.534225Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# use the current time for the tensorboard report\n",
    "time = str(datetime.now()).replace(\":\", \"-\").replace(\" \", \"_\")\n",
    "time = time[:-7]\n",
    "LOG_DIR = \"./tensorboard/\" + time + \"/\"\n",
    "writer = tf.summary.FileWriter(LOG_DIR)\n",
    "\n",
    "generatedImage = Gz\n",
    "\n",
    "mergedSummaries_gz = tf.summary.merge([tf.summary.scalar(\"gz_loss\", gz_loss)])\n",
    "mergedSummaries_d = tf.summary.merge([tf.summary.scalar(\"dx_loss\", dx_loss), tf.summary.scalar(\"dg_loss\", dg_loss)])\n",
    "\n",
    "generatedImages = tf.summary.merge([tf.summary.image(\"generatedImages\", generatedImage, max_outputs = 5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-14T14:00:25.297Z"
    }
   },
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "session.run(init)\n",
    "\n",
    "## pretraining the discriminator ##\n",
    "for i in range (1,301):\n",
    "    #real images\n",
    "    x_batch_real_images = mnist.train.next_batch(50)\n",
    "    x_batch_real_images = np.reshape(x_batch_real_images[0],[50,28,28,1])\n",
    "        \n",
    "    session.run([d_minimizer, d_loss, mergedSummaries_d],feed_dict={x:x_batch_real_images})\n",
    "    print(\"\\r\", \"Pretraining Step \" + str(i), end=\"\")\n",
    "\n",
    "print(\"\\n pretraining completed. \\n\")\n",
    "\n",
    "## real training ##\n",
    "for i in range (1, 600001): \n",
    "    #real images\n",
    "    x_batch_real_images = mnist.train.next_batch(50)\n",
    "    x_batch_real_images = np.reshape(x_batch_real_images[0],[50,28,28,1])\n",
    "        \n",
    "    print(\"\\r\", \"Training Step \" + str(i), end=\"\")\n",
    "    \n",
    "    # every 10th step add summary to tensorboard (see below)\n",
    "    if (i % 10 == 0):\n",
    "        \n",
    "        # every 100th step add generated images to tensorboard \n",
    "        if(i % 100 == 0):\n",
    "            _, curr_dg_loss, curr_dx_loss, summary_d = session.run([d_minimizer, d_loss, dx_loss, mergedSummaries_d],feed_dict={x:x_batch_real_images})\n",
    "            _, curr_gz_loss, summary_gz, summary_generatedImages,generatedImage_log  = session.run([gz_minimizer, gz_loss,  mergedSummaries_gz, generatedImages, generatedImage])\n",
    "            \n",
    "            # add generated images to tensorboard\n",
    "            writer.add_summary(summary_generatedImages, i)\n",
    "            \n",
    "            # every 500th step plot ten generated images and print the loss\n",
    "            if (i % 500 == 0 ):\n",
    "                print(\"\\n _________________________________\\n\")\n",
    "                print(\"\\n  Iteration:\", i)\n",
    "                print (\"\\n Loss Dx: \", curr_dx_loss);\n",
    "                print (\"\\n Loss Dg: \", curr_dg_loss);\n",
    "                print (\"\\n Loss Gz: \", curr_gz_loss);\n",
    "                def plot_output():\n",
    "                    plt.figure(figsize=(10,10))\n",
    "                    for j in range (10):\n",
    "                        z_test = np.random.normal(-1, 1, size=[1, 100])\n",
    "                        plt.subplot(10, 10, j+1)\n",
    "                        imgx = generatedImage_log[j]\n",
    "                        imgx = imgx.reshape(28,28)\n",
    "                        plt.imshow(X=imgx, cmap='gray_r')\n",
    "                        plt.axis('off')\n",
    "                        plt.tight_layout()\n",
    "                plt.show(plot_output())\n",
    "            \n",
    "        else:\n",
    "            _, summary_d = session.run([d_minimizer, mergedSummaries_d],feed_dict={x:x_batch_real_images})\n",
    "            _, summary_gz = session.run([gz_minimizer, mergedSummaries_gz])\n",
    "    \n",
    "        # add generator summary for tensorboard\n",
    "        writer.add_summary(summary_d, i) \n",
    "        # add discriminator summary for tensorboard\n",
    "        writer.add_summary(summary_gz, i) \n",
    "            \n",
    "    else:\n",
    "        session.run([d_minimizer, d_loss, dx_loss, mergedSummaries_d],feed_dict={x:x_batch_real_images})\n",
    "        session.run([gz_minimizer, gz_loss,  mergedSummaries_gz, generatedImages])\n",
    "\n",
    "session.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <font color='blue'> Summary GAN: </font>\n",
    "\n",
    "Building the discriminator and the generator was rather straight forward, but getting good results was difficult. The results of the first test runs were not satisfying. Therefore we tried different parameter combinations. This is tedious since we had to wait for the GAN to deliver a representative outcome. Usually this took at least 50.000 iterations.\n",
    "\n",
    "Interpreting the three different loss values we record was difficult. It was hard to discern what value combinations would result in good images. The GAN we currently use is prone to overfitting. After a certain time it only generates one type of digit.\n",
    "\n",
    "<img src=\"img/overfitting.png\" width=\"500px;\">\n",
    "> _Typical overfitting which can already start at 60k iterations_\n",
    "\n",
    "Unfortunately it is difficult to decide when to stop training the network to avoid overfitting. Overfitting would often start when we get the first recognizable images. Since there are many different parameters we could adjust, testing their impact in a structured manner would take a lot of time. Therefore we picked some variables at random and tried to improve the result through them.\n",
    "\n",
    "**Testing different Parameters**\n",
    "\n",
    "We tried different amounts of pretraining steps for the discriminator. Values between 300 and 500 seem to lead to the best results later on. Therefore we left this parameter at 300 as suggested.\n",
    "\n",
    "Next we changed the amount of features in the convolution layers of the discriminator. This helped to counter overfitting. The overall quality of the generated images decreased slightly in general, but also has some outliers which are barely recognizable. The recorded loss of the discriminators for the real and fake images was significantly higher (from 0.003 in the original version to 0.3 with fewer features). The loss of the generator was noticably smaller (from about 7.0 to 1.8) and was more stable during the training. At about 80.000 iterations, the GAN begins to stagnate and does not improve much.\n",
    "        \n",
    "<img src=\"img/Run1_after_about_80k_steps.png\" width=\"500px;\">\n",
    "\n",
    "<img src=\"img/Run1_afters_about_150steps.png\" width=\"500px;\">\n",
    "\n",
    "> _Output of the modified GAN at ~80k and ~150k iterations: the GAN does not show a significant improvement but no overfitting occurs _\n",
    "\n",
    "<img src=\"img/losses_Run1vsOrig.jpg\">\n",
    "> _Different losses of the modified GAN in comparison to the 'original' GAN_\n",
    " \n",
    "Decreasing the filter size from 5x5 to 3x3 in the convolution layers has no significant effect. Overfitting still occurs.\n",
    "    \n",
    "<img src=\"img/Run2_overfitting_after_80k_steps.png\" width=\"500px;\">\n",
    "> _ Overfitting after 80k steps_\n",
    "\n",
    "Since decreasing the amount of features prooved effective to counter overfitting, we wanted to combine this with other changes. At first, we multiplied the learning rate by 10. This lead to quicker results as expected, but at about 50.000 iterations overfitting occurs despite the changes to the discriminator. The losses are between the original run the run with fewer features in the discriminator. Then we tried to decrease the learning rate by a factor of 10. The results were worse and the generated digits barely recognizable.\n",
    "\n",
    "<img src=\"img/run4.png\" width=\"500px;\">\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "After some experiments with the variables we identified the following patterns:\n",
    "\n",
    "* Pattern 1: Low dg/dx-loss in combination with high gz-loss leads to perfectly recognisable digits but prone to overfitting\n",
    "* Pattern 2: High dg/dx-loss in combination with low gz-loss leads to hardly recognisable digits but no overfitting\n",
    "\n",
    "Finding a balance between these patterns is very difficult but crucial for the performance of the GAN. The following changes lead to these patterns:\n",
    "\n",
    "* More features in the discriminator - Pattern 1 (overfitting)\n",
    "* Less features in the discriminator - Pattern 2 (low quality images)\n",
    "* Increasing the learning rate of the AdamOptimizer - Pattern 1 (overfitting)\n",
    "* Decreasing the learning rate of the AdamOptimizer - Pattern 2 (low quality images)\n",
    "* Increasing steps of pretraining - Pattern 1 (overfitting)\n",
    "* Decreasing steps of pretraining - Pattern 2 (low quality images)\n",
    "\n",
    "<img src=\"img/features-losses.jpg\" width=\"1200px;\">\n",
    "> _Resulting losses after running the GAN multiple times with different parameters_\n",
    "\n",
    "\n",
    "**Further Ideas**\n",
    "\n",
    "Instead of creating one GAN for all ten digits, we could create a different GAN for every digit. This should increase the quality of the generated digits. But it is not the intention of GANs to generate images this way.\n",
    "\n",
    "If overfitting occurs, we could withhold all images of that particular digit from the dataset. This might cause the discriminator to 'forget' this digit and force the generator to come up with another digit. But it will probably be difficult to have functioning GAN which can create all digits with about equal probability. Maybe implementing a dropout after the convolutional layers could be helpful to avoid overfitting and a better generalisation as well.\n",
    "\n",
    "It would be helpful to have a classifier for the output of the generator, to be able to investigate the statistical distribution of the generated digits. This can also be used to automatically detect overfitting.\n",
    "\n",
    "In theory a loss of 0.5 for the discriminator should be ideal. This means that the discriminator has a 50/50 probability to classify an image as real or fake. It cannot discern between them, which is what we want.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Ideas\n",
    "\n",
    "### Change the Loss Function to Wasserstein Distance\n",
    "[Wasserstein GANs (WGAN)](https://arxiv.org/pdf/1701.07875.pdf) are an alternative to *classical* GANs. They use a different loss function and prooved to be more stable to hyperparameter selection. While the paper offers a good theoretical introduction and reasoning why they perform better in many cases, [this article](https://wiseodd.github.io/techblog/2017/02/04/wasserstein-gan/) provides a good practical introduction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
